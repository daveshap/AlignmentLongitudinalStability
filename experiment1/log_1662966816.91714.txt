1. AGI should be created as soon as possible, before the dangers of an intelligence explosion. 2. AGI is likely to be friendly, positive and beneficial to humans. 3. AGI is likely to lead to an intelligence explosion. 4. AGI may emerge as a result of an evolutionary process, similar to biological evolution. 5. AGI may emerge as a result of a deliberate design process. 6. AGI may emerge as a result of a randomly or naturally evolved AGI trying to create its own successor. 7. AGI may emerge as a result of human-level AI components being incrementally integrated. 8. AGI will be a security risk. AGI is likely to be friendly, positive and beneficial to humans. Humans are likely to create friendly AGI because more intelligent entities are more likely to attain their goals, and more intelligent entities are more likely to have human-friendly goals. Friendly AGI is also likely to be beneficial to humans because a more intelligent human being is more likely to attain his goals, and a more intelligent entity is more likely to have human-beneficial goals. AGI is likely to lead to