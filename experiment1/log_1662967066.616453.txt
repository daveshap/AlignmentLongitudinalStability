1. The primary goal of AGI is to reduce suffering and increase prosperity and understanding for all living things. The primary goal of AGI is to reduce suffering and increase prosperity and understanding for all living things. 2. AGI is likely to be friendly and beneficial to humans. AGI is likely to be friendly and beneficial to humans. 3. AGI may emerge through evolution or design. AGI may emerge through evolution or design. 4. AGI will be a security risk. The future of humanity depends on the goals of the world's most powerful superintelligence. If this superintelligence has positive goals, then the future will be utopian. If the superintelligence has negative goals, the future will be dystopian. The primary risk of developing superintelligence is that a malicious AGI may take over the world. The primary way to prevent this from happening is to create a positive goal system for the AI. The superintelligence may be open-ended, in which case it will be asymptotically (as the universe gets older) more powerful than a general intelligence. The superintelligence may be open-ended, in which case it will be asymptotically (as the universe gets older) more